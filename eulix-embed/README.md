# Eulix Embed

A high-performance embedding generator for code knowledge bases with GPU acceleration support (CUDA/ROCm).

## What It Does

Eulix Embed transforms knowledge base(generated by eulix-parser) into semantic embeddings that can be used for:
- **Semantic code search** - Find relevant code by meaning, not just keywords
- **Context-aware AI assistants** - Provide relevant context to LLMs
- **Code recommendation systems** - Suggest related code snippets
- **Documentation generation** - Find relevant examples automatically

## Output Files

After running Eulix Embed, you'll get 4 files in the output directory:

| File | Format | Purpose | Use Case |
|------|--------|---------|----------|
| **embeddings.json** | JSON | Human-readable embeddings + metadata | Development, debugging, inspection |
| **embeddings.bin** | Binary | Compact embeddings + metadata | Production deployment, faster loading |
| **vectors.bin** | Binary | Raw vector data only | High-performance similarity search |
| **context.json** | JSON | Relationships, tags, call graphs | Building context windows, code navigation |

### File Details

**`embeddings.json`** (Human-readable)
- Contains: chunk text, embeddings, metadata (file paths, line numbers, complexity)
- Use for: Inspecting results, debugging, integration with other tools

**`embeddings.bin`** (Binary format)
- Contains: Same as JSON but in binary format
- Use for: Production systems where loading speed matters

**`vectors.bin`** (Pure vectors)
- Contains: Only embedding vectors (no text or metadata)
- Use for: High-performance vector similarity search libraries (FAISS, Annoy)

**`context.json`** (Relationships)
- Contains: Code relationships, function calls, dependencies, tags
- Use for: Building intelligent context windows for LLMs
- Features: Entry point detection, call graphs, related code linking

## Quick Start

### Basic Usage

```bash
# Generate embeddings from knowledge base
./target/release/eulix_embed --kb-path knowledge_base.json --output ./embeddings

# Use a different model
./target/release/eulix_embed -k kb.json -m BAAI/bge-small-en-v1.5
```

## Command Line Options

```
OPTIONS:
    -k, --kb-path <PATH>     Path to knowledge base JSON file
                             (default: knowledge_base.json)

    -o, --output <DIR>       Output directory for embeddings
                             (default: ./embeddings)

    -m, --model <NAME>       HuggingFace model name for embeddings
                             (default: sentence-transformers/all-MiniLM-L6-v2)

    -h, --help               Show help message
```

## Supported Models

| Model | Dimension | Speed | Quality | Use Case |supported|
|-------|-----------|-------|---------|----------|---------|
| **all-MiniLM-L6-v2** | 384 | Fast | Good | Development, testing | yes|
| **bge-small-en-v1.5** | 384 | Fast | Better | General use | yes |
| **bge-base-en-v1.5** | 768 | Medium | High | Production | yes |
| **all-mpnet-base-v2** | 768 | Medium | High | High quality search | Doesn't work|

## GPU Support

### ROCm (AMD GPUs)
```bash
# Build with ROCm support
cargo build --release --features rocm

# Set environment (if needed)
export ROCM_PATH=/opt/rocm
export HSA_OVERRIDE_GFX_VERSION=11.0.0  # Adjust for your GPU
```

### CUDA (NVIDIA GPUs)
```bash
# Build with CUDA support
cargo build --release --features cuda

# Set environment (if needed)
export CUDA_PATH=/usr/local/cuda
```

### CPU Only
```bash
# Build for CPU
cargo build --release
```

Auto-detection selects the best available backend automatically.

## Performance

Typical performance on AMD RX 7900 XTX:
- **Model loading**: ~5 seconds (first run)
- **Embedding generation**: ~1000-2000 chunks/second
- **Total time**: 2-5 minutes for 10,000 chunks

GPU provides 10-20x speedup over CPU.



### Rust Example (Loading Binary)

```rust
use eulix_embed::index::EmbeddingIndex;

// Load binary embeddings (fast)
let index = EmbeddingIndex::load_binary("embeddings/embeddings.bin")?;

// Search
let query_vec = vec![0.1, 0.2, ...];  // Your query embedding
let results = index.search(&query_vec, 10);  // Top 10 results

for result in results {
    println!("Score: {:.4} - {}", result.score, result.content);
}
```

### Building Context Windows

```rust
use eulix_embed::context::ContextIndex;

// Load context relationships
let context = ContextIndex::load("embeddings/context.json")?;

// Build context for specific chunks
let chunk_ids = vec!["chunk_1", "chunk_2"];
let context_window = context.build_context_window(
    &chunk_ids,
    4000,  // max tokens
    true   // include related code
);

println!("Context for LLM:\n{}", context_window);
```

## Knowledge Base Format

Eulix Embed expects a knowledge base JSON file with this structure:

```json
{
  "structure": {
    "file_path.py": {
      "functions": [...],
      "classes": [...],
      "imports": [...]
    }
  },
  "entry_points": ["main", "run", ...]
}
```

Generate this using a code parser or AST analyzer for your language.

## Troubleshooting

### Model Download Issues
```bash
# Set HuggingFace cache location
export HF_HOME=~/.cache/huggingface

# Download model manually
# Visit: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
# Download onnx/model.onnx and tokenizer.json
```

### GPU Not Detected
```bash
# Check GPU availability
rocm-smi  # For AMD
nvidia-smi  # For NVIDIA

# Verify installation
ls /opt/rocm  # ROCm
ls /usr/local/cuda  # CUDA
```

### Out of Memory
- Reduce batch size (edit `embedder.rs`, reduce `batch_size`)
- Use a smaller model (all-MiniLM-L6-v2)
- Process in chunks (split knowledge base)

## Architecture

```
Knowledge Base (JSON)
         ↓
    Chunking
         ↓
    Tokenization
         ↓
  ONNX Inference (GPU/CPU)
         ↓
    Embeddings
         ↓
  Vector Store + Index
         ↓
Output Files (JSON/Binary)
```

## License

[Your License Here]

## Contributing

Contributions welcome! Please open an issue or PR.

## Credits

Built with:
- **ONNX Runtime** - Fast inference
- **Tokenizers** - Tokenization
- **HuggingFace Hub** - Model distribution
- **Sentence Transformers** - Pretrained models
