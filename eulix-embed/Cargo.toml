[package]
name = "eulix_embed"
version = "0.5.0"
edition = "2021"

[[bin]]
name = "eulix_embed"
path = "src/main.rs"

[dependencies]
# Core dependencies
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
chrono = { version = "0.4", features = ["serde"] }

# Parallel processing
rayon = "1.10"

# CLI
clap = { version = "4.5", features = ["derive"] }

# Logging
env_logger = "0.11"
log = "0.4"

# Tokenizers
tokenizers = { version = "0.19", optional = true }

# Candle dependencies (optional, enabled by default)
candle-core = { git = "https://github.com/huggingface/candle", optional = true }
candle-nn = { git = "https://github.com/huggingface/candle", optional = true }
candle-transformers = { git = "https://github.com/huggingface/candle", optional = true }

# HuggingFace Hub for downloading models
#hf-hub = { version = "0.3", default-features = false, features = ["ureq"], optional = true }
hf-hub = { version = "0.4", default-features = false, features = ["ureq"], optional = true }

[features]
default = ["candle-cpu"]

# Candle backend with CPU only
candle-cpu = [
    "candle-core",
    "candle-nn",
    "candle-transformers",
    "tokenizers",
    "hf-hub"
]

# CUDA support for NVIDIA GPUs
cuda = [
    "candle-core",
    "candle-nn",
    "candle-transformers",
    "candle-core/cuda",
    "candle-nn/cuda",
    "tokenizers",
    "hf-hub"
]

# ROCm support for AMD GPUs
# Note: Candle uses 'cuda' feature name for both CUDA and ROCm
# The actual backend is selected at runtime
rocm = [
    "candle-core",
    "candle-nn",
    "candle-transformers",
    "candle-core/cuda",  # ROCm uses the cuda feature
    "candle-nn/cuda",
    "tokenizers",
    "hf-hub"
]

# All GPU features (currently same as CUDA)
gpu = ["cuda"]

[dev-dependencies]
criterion = "0.5"

[profile.release-with-debug]
inherits = "release"
strip = false
debug = true

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
strip = true

[profile.dev]
opt-level = 1
